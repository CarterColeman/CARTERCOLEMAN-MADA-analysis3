---
title: "analysis3_part2"
author: "C. Coleman"
date: "10/19/2021"
output: html_document
---

# Fit model analysis!

_In this section, we will be using the clean_df data to split, train, and assess the fit model of flu-like symptom data completed in the last module._

_As such, we will be using the Tidymodels package to continue practicing data analysis._

</br>

_As always, I like to start by loading needed packages. Our core packages for this specific exercise will be here(), dpylr(), and tidymodels(). Additionally, I like to load tidyverese() just because I use it fairly often._
```{r}
library(tidyverse)
library(tidymodels)
library(dplyr)
library(here)
```


## Data Splitting and Training

</br>

_Next, we need to load in out data. We are going to pull from the processed data folder using the here() function. Note that while we are reading in the clean_df data frame, we will be calling it "df" in this exercise._
```{r}
clean_df_location <- here::here("data", "processed_data", "processeddata.RDS")

df <- readRDS(clean_df_location)
```

</br>

 _Now we are set to start our data splitting/training._
 
 _The first step is splitting our data into a training data set and a test data set. The training set will contain most of the original data (~75%), while the test data set will have a smaller portion (~25%). The training data is used to fit a model and the test data is to assess how good of a fit the data is._
```{r}
# Split 3/4 of the data into training data 
data_split <- initial_split(df, prop = 3/4)

# Make new data frames for training and test data
train_data <- training(data_split)
test_data  <- testing(data_split)
```
 
 </br>
 
## Assessing fit model for Nausea and all other outcomes.
 
 </br>
 
#### Creating a Recipe and Workflow

</br>

_At this stage, our data is split into training and test data sets. Now, we need to create a recipe and workflow to help process the train data for model building. The output is a function that will run the entire logistic regression model for any data set. Therefore, we can run the same model for the train and test data frames to create the exact same analysis workflow to ensure comparison_
```{r}
#Creates recipe
Recipe_Nausea <- recipe(Nausea ~ ., data = train_data)

#Define logistical regression model pipe
log_mod <- logistic_reg() %>% 
  set_engine("glm")

#Create workflow that adds our recipe and model
Nausea_glm_wflow <- 
  workflow() %>% 
  add_model(log_mod) %>% 
  add_recipe(Recipe_Nausea)
```

</br>

#### Modeling Using the Workflow

</br>

_Using the workflow above, lets not fit the model to the train data set (Recipe_Nausea)._
```{r}
#Defining a command that runs model fitting to Recipe_Nausea
Nausea_fit <- 
  Nausea_glm_wflow %>% 
  fit(data = train_data)

#Pull log reg fit model using parsnip()
Nausea_fit %>% 
  extract_fit_parsnip() %>% 
  tidy()
```

</br>

### Using Workflows to Make Predictions

</br>

_Now that we have our modeling workflow, we are going to use the fitted model to predict values in the test data set._
```{r}
#predict values in test data
predict(Nausea_fit, test_data)
```

</br>

_Additionally, we can use the augment() function to predict outcomes in the test data. But unlike the predict() function, augment() indludes prediction residuals._
```{r}
#Predict outcomes using augment in test data
Nausea_aug_test <- 
  augment(Nausea_fit, test_data)
```

</br>

### Using ROC and ROC_AUC to Assess Model Fit

</br>

_At this point, we have a fit model, a workflow for that model, and have some predictions made from a test subset of our data. Now, we need to used to test predictions to assess if the model can predict values that reflect our actual data. In particular, we will be using the the *ROC* curve as our metric with *ROC_AUC* to measure area under the ROC curve that our model intersects._

#### Assessing fit for test data predictions

</br>
```{r}
#Make ROC curve for test data predictions and calculate area under the curve
Nausea_aug_test %>% 
  roc_curve(truth = Nausea, .pred_Yes, event_level = "second") %>% 
  autoplot()

Nausea_aug_test %>%
  roc_auc(truth = Nausea, .pred_Yes, event_level = "second")
```
</br>

```{r}
#Predict outcomes using augment in training data
Nausea_aug_train <- 
  augment(Nausea_fit, train_data)

#Make ROC curve for training data and calculate area under the curve
Nausea_aug_train %>% 
  roc_curve(truth = Nausea, .pred_Yes, event_level = "second") %>% 
  autoplot()

Nausea_aug_train %>%
  roc_auc(truth = Nausea, .pred_Yes, event_level = "second")
```

</br>

_The resulting ROC_AUC values were 0.68 and 0.79 for the test data model and the training data model, respectively. This suggests that the models are good fits to our data and can predict outcomes._

</br>

### Assessing fit model for Nausea and Runny Nose.

 </br>
 
#### Creating a Recipe and Workflow

</br>

_Create a recipe and workflow to help process the train data for model building._
```{r}
#Creates recipe
Nausea_RN_Recipe <- recipe(Nausea ~ RunnyNose, data = train_data)

#Define logistical regression model pipe
log_mod <- logistic_reg() %>% 
  set_engine("glm")

#Create workflow that adds our recipe and model
Naus_RN_glm_wflow <- 
  workflow() %>% 
  add_model(log_mod) %>% 
  add_recipe(Nausea_RN_Recipe)
```

</br>

#### Modeling Using the Workflow

</br>

_Use workflow to fit the model to the train data set (Nausea_RN_Recipe)._
```{r}
#Defining a command that runs model fitting to Nausea_RN_Recipe
Nausea_RN_fit <- 
  Naus_RN_glm_wflow %>% 
  fit(data = train_data)

#Pull log reg fit model using parsnip()
Nausea_RN_fit %>% 
  extract_fit_parsnip() %>% 
  tidy()
```

</br>

### Using Workflows to Make Predictions

</br>

_Use the fitted model to predict values in the test data set._
```{r}
#predict values in test data
predict(Nausea_RN_fit, test_data)
```

</br>

_Use the augment() function to predict outcomes in the test data._
```{r}
#Predict outcomes using augment in test data
Nausea_RN_aug_test <- 
  augment(Nausea_RN_fit, test_data)
```

</br>

### Using ROC and ROC_AUC to Assess Model Fit

</br>

#### Assessing fit for test data predictions

</br>
```{r}
#Make ROC curve for test data predictions and calculate area under the curve
Nausea_RN_aug_test %>% 
  roc_curve(truth = Nausea, .pred_Yes, event_level = "second") %>% 
  autoplot()

Nausea_RN_aug_test %>%
  roc_auc(truth = Nausea, .pred_Yes, event_level = "second")
```
</br>

```{r}
#Predict outcomes using augment in training data
Nausea_RN_aug_train <- 
  augment(Nausea_RN_fit, train_data)

#Make ROC curve for training data and calculate area under the curve
Nausea_RN_aug_train %>% 
  roc_curve(truth = Nausea, .pred_Yes, event_level = "second") %>% 
  autoplot()

Nausea_RN_aug_train %>%
  roc_auc(truth = Nausea, .pred_Yes, event_level = "second")
```

</br>

_The resulting ROC_AUC values were 0.48 and 0.52. for the test data model and the training data model, respectively. This suggests that the test data was not fit well by the model, but the model had a decent fit to train data, but not a great fit. Therefore, since both ROC_AUC values are ~0.5, it is okay to make predictions using the model, but use caution._
